{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "seed_val = 2218\n",
        "np.random.seed(seed_val)\n",
        "tf.random.set_seed(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-3kEaggcO8",
        "outputId": "fae63559-5f5e-4386-fb78-b4484e94505b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw78jw6pDqSM"
      },
      "source": [
        "#Get number of Gpu's and id's in the system or else you can also use Nvidia-smi in command prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNF2gLLxARl-",
        "outputId": "607e35db-2753-4268-b007-cae350332a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#downloading the dataset\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "40XlFnwho7D8"
      },
      "outputs": [],
      "source": [
        "size_input = train_images[0].shape[0] * train_images[0].shape[1]\n",
        "size_hidden = 256\n",
        "size_hidden_2 = 128\n",
        "size_output = 10\n",
        "learning_rate_hp = 1e-3\n",
        "number_of_train_examples = train_images.shape[0]\n",
        "number_of_test_examples = test_images.shape[0]\n",
        "train_batch = 60\n",
        "test_batch = 10\n",
        "lamda = 0.7\n",
        "NUM_EPOCHS = 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DLFMCfKDIcAs"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(vec):\n",
        "  n = len(vec)\n",
        "  out = np.zeros((n, 10))\n",
        "  for i in range(n):\n",
        "    out[i, vec[i]] = 1\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm23CzRihaW0",
        "outputId": "fbefe258-9960-4699-d51b-fe70884f7668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)   (60000, 10)\n"
          ]
        }
      ],
      "source": [
        "X_train = train_images.reshape(train_images.shape[0], -1)\n",
        "X_test = test_images.reshape(test_images.shape[0], -1)\n",
        "\n",
        "y_train = np.reshape(train_labels, (number_of_train_examples, 1))\n",
        "y_test = np.reshape(test_labels, (number_of_test_examples, 1))\n",
        "y_train = one_hot_encode(y_train)\n",
        "y_test = one_hot_encode(y_test)\n",
        "\n",
        "print(X_train.shape, \" \", y_train.shape)\n",
        "\n",
        "#standardizing the images\n",
        "\n",
        "x_train = X_train / 255.0\n",
        "x_test = X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vdAku19sLilG"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(mlp, X_test, y_test):\n",
        "  '''\n",
        "  This function does a forward pass of x, then checks if the indices\n",
        "  of the maximum value in the output equals the indices in the label \n",
        "  y. Then it sums over each prediction and calculates the accuracy.\n",
        "  '''\n",
        "  predictions = []\n",
        "  # for x, y in zip(X_test, y_test):\n",
        "  #   x = tf.reshape(x, (1, size_input))\n",
        "  #   output = mlp.forward(x)\n",
        "  #   pred = np.argmax(output)\n",
        "  #   predictions.append(pred == np.argmax(y))\n",
        "\n",
        "  output = mlp.forward(X_test)\n",
        "  pred = np.argmax(output, axis=1)\n",
        "  predictions.append(pred == np.argmax(y_test, axis=1))\n",
        "\n",
        "  return np.mean(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ht9_qpYipgHw"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_hidden_2, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_hidden_2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_hidden_2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "     # Initialize weights between input layer and hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden_2]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden_2]))\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden_2, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    l2_rg = (lamda / (2.0 * number_of_train_examples)) * (np.sum(np.square(self.variables[0])) + np.sum(np.square(self.variables[1])) + np.sum(np.square(self.variables[2])))\n",
        "    #l1_rg = (lamda / number_of_train_examples) * (np.sum(self.variables[0]) + np.sum(self.variables[1]) + np.sum(self.variables[2]))\n",
        "    return tf.losses.categorical_crossentropy(y_true_tf, y_pred_tf) + l2_rg\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #print(self.variables)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_hp)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "      # L2 Regularization\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer 1\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.sigmoid(what)\n",
        "    # Compute values in hidden layer 2\n",
        "    what_2 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat_2 = tf.nn.sigmoid(what_2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat_2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moAeRMJ56kr6",
        "outputId": "5d2ffdb4-294d-4e59-c56b-7f59fa9373ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average Error:= 11.47461875 - Accuracy:= 9.12%\n",
            "Number of Epoch = 101 - Average Error:= 2.25425546875 - Accuracy:= 68.96%\n",
            "Number of Epoch = 201 - Average Error:= 2.0086016927083334 - Accuracy:= 74.17%\n",
            "Number of Epoch = 301 - Average Error:= 1.91281640625 - Accuracy:= 76.30%\n",
            "Number of Epoch = 401 - Average Error:= 1.8549825520833334 - Accuracy:= 77.55%\n",
            "Number of Epoch = 501 - Average Error:= 1.8192165364583333 - Accuracy:= 78.13%\n",
            "Number of Epoch = 601 - Average Error:= 1.794733203125 - Accuracy:= 78.45%\n",
            "Number of Epoch = 701 - Average Error:= 1.7754037760416668 - Accuracy:= 78.62%\n",
            "Number of Epoch = 801 - Average Error:= 1.7615903645833333 - Accuracy:= 78.98%\n",
            "Number of Epoch = 901 - Average Error:= 1.7499489583333334 - Accuracy:= 79.15%\n",
            "Number of Epoch = 1001 - Average Error:= 1.74370625 - Accuracy:= 79.28%\n",
            "Number of Epoch = 1101 - Average Error:= 1.7323908854166667 - Accuracy:= 79.29%\n",
            "Number of Epoch = 1201 - Average Error:= 1.7257450520833333 - Accuracy:= 79.58%\n",
            "Number of Epoch = 1301 - Average Error:= 1.718266015625 - Accuracy:= 79.71%\n",
            "Number of Epoch = 1401 - Average Error:= 1.7128670572916667 - Accuracy:= 79.91%\n",
            "Number of Epoch = 1501 - Average Error:= 1.7088791666666667 - Accuracy:= 80.10%\n",
            "Number of Epoch = 1601 - Average Error:= 1.7038481770833334 - Accuracy:= 80.09%\n",
            "Number of Epoch = 1701 - Average Error:= 1.7014140625 - Accuracy:= 80.14%\n",
            "Number of Epoch = 1801 - Average Error:= 1.6993255208333333 - Accuracy:= 80.14%\n",
            "Number of Epoch = 1901 - Average Error:= 1.6971627604166666 - Accuracy:= 80.23%\n",
            "Number of Epoch = 2001 - Average Error:= 1.6980610677083334 - Accuracy:= 80.46%\n",
            "Number of Epoch = 2101 - Average Error:= 1.696020703125 - Accuracy:= 80.32%\n",
            "Number of Epoch = 2201 - Average Error:= 1.6967751302083334 - Accuracy:= 80.60%\n",
            "Number of Epoch = 2301 - Average Error:= 1.6965536458333332 - Accuracy:= 80.50%\n",
            "Number of Epoch = 2401 - Average Error:= 1.6953055989583334 - Accuracy:= 80.60%\n",
            "Number of Epoch = 2501 - Average Error:= 1.695484765625 - Accuracy:= 80.71%\n",
            "Number of Epoch = 2601 - Average Error:= 1.695603125 - Accuracy:= 80.48%\n",
            "Number of Epoch = 2701 - Average Error:= 1.6960352864583332 - Accuracy:= 80.61%\n",
            "Number of Epoch = 2801 - Average Error:= 1.6969502604166666 - Accuracy:= 80.68%\n",
            "Number of Epoch = 2901 - Average Error:= 1.6995822916666667 - Accuracy:= 80.54%\n",
            "Number of Epoch = 3001 - Average Error:= 1.7000619791666667 - Accuracy:= 80.68%\n",
            "\n",
            "Total time taken (in seconds): 359.91\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.ops.nn_impl import compute_average_loss\n",
        "# Initialize model using GPU\n",
        "mlp = MLP(size_input, size_hidden, size_hidden_2, size_output, device='gpu')\n",
        "error = []\n",
        "accuracy = []\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS + 1):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  # train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(train_batch)\n",
        "\n",
        "  preds = mlp.forward(X_train)\n",
        "  loss_total = loss_total + mlp.loss(preds, y_train)\n",
        "  lt = lt + mlp.loss(preds, y_train)\n",
        "  gd = mlp.backward(X_train, y_train)\n",
        "  '''\n",
        "  This code is for batch gradient descent\n",
        "  '''\n",
        "  '''\n",
        "  mlp.vanillasgd(gd)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp.forward(inputs)\n",
        "    #print(inputs.shape, \" \", outputs.shape, \" \", preds.shape)\n",
        "    loss_total = loss_total + mlp.loss(preds, outputs)\n",
        "    lt = lt + mlp.loss(preds, outputs)\n",
        "    gd = mlp.backward(inputs, outputs)\n",
        "    #mlp.vanillasgd(gd)\n",
        "  '''\n",
        "  curr_error = np.sum(loss_total) / X_train.shape[0]\n",
        "  error.append(curr_error)\n",
        "  curr_accuracy = compute_accuracy(mlp, X_test, y_test) * 100\n",
        "  accuracy.append(curr_accuracy)\n",
        "  if(epoch % 100 == 0):\n",
        "    print('Number of Epoch = {} - Average Error:= {} - Accuracy:= {:.2f}%'.format(epoch + 1, curr_error, curr_accuracy))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZV7iJa77kJjq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "9a5aa80b-3199-40a9-a730-38ef3528bbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted output:=  3\n",
            "Actual ouput:=  3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5642ab0710>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "test_index = 300\n",
        "test_data = tf.reshape(X_test[test_index], (1, size_input))\n",
        "output = mlp.forward(test_data)\n",
        "pred = np.argmax(output, axis=1)\n",
        "print(\"Predicted output:= \", int(pred))\n",
        "print(\"Actual ouput:= \", test_labels[test_index])\n",
        "plt.imshow(test_images[test_index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bZqEb_-6kOW0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "9a504019-68f1-4d0a-d81a-ac0f5ae70b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.120000000000001, 9.87, 10.639999999999999, 11.37, 12.32, 13.170000000000002, 14.63, 15.57, 17.080000000000002, 17.849999999999998, 19.17, 20.96, 21.45, 22.509999999999998, 23.82, 25.1, 26.900000000000002, 28.199999999999996, 30.28, 31.94, 33.81, 35.23, 36.69, 38.15, 39.17, 40.08, 41.65, 42.699999999999996, 43.9, 44.99, 45.87, 47.15, 47.94, 48.76, 49.519999999999996, 50.4, 51.23, 51.59, 52.190000000000005, 53.06999999999999, 53.349999999999994, 54.35, 54.620000000000005, 55.32, 55.57, 56.43, 56.49999999999999, 57.220000000000006, 57.699999999999996, 58.02, 58.45, 58.589999999999996, 59.06, 59.06, 59.540000000000006, 59.88, 59.89, 60.440000000000005, 60.56, 61.1, 61.260000000000005, 61.79, 62.08, 62.419999999999995, 62.81, 62.83, 63.38, 63.29, 63.43, 63.44, 63.73, 63.89, 64.53, 64.25999999999999, 64.8, 65.02, 65.56, 65.52, 65.77, 65.55, 65.86, 65.8, 66.60000000000001, 66.57, 66.78, 66.97, 67.06, 67.30000000000001, 67.35, 67.42, 67.64, 67.69, 67.83, 67.85, 67.96, 67.65, 68.28, 68.15, 68.57, 68.17, 68.96, 68.54, 69.19, 68.77, 69.34, 69.0, 69.53, 69.5, 69.89999999999999, 69.61, 69.71000000000001, 69.6, 69.89999999999999, 69.88, 70.12, 70.06, 70.00999999999999, 70.47, 70.72, 70.66, 70.56, 70.84, 70.41, 70.78, 70.86, 70.83, 71.08, 71.05, 71.11, 71.11, 71.28999999999999, 71.08, 71.27, 71.02000000000001, 71.37, 71.1, 71.39, 71.25, 71.46000000000001, 71.7, 71.72, 71.61, 71.67, 71.75, 72.27, 71.71, 72.19, 72.02, 72.02, 72.05, 72.17, 72.19, 72.25, 72.37, 72.63, 72.46000000000001, 72.66, 72.54, 72.57000000000001, 72.21, 72.38, 72.44, 72.72, 72.92, 72.7, 72.95, 73.06, 73.04, 73.05, 72.88, 73.03, 73.15, 73.18, 73.32, 73.46000000000001, 73.54, 73.71, 73.31, 73.49, 73.44000000000001, 73.57000000000001, 73.42999999999999, 73.56, 73.85000000000001, 73.50999999999999, 73.58, 73.83, 73.68, 73.78, 73.50999999999999, 73.76, 73.83999999999999, 74.26, 73.85000000000001, 73.92999999999999, 73.6, 74.24, 74.00999999999999, 73.91, 73.87, 74.17, 73.72999999999999, 74.05000000000001, 74.21, 74.24, 74.19, 74.32, 74.28, 74.4, 74.29, 74.59, 74.41, 74.55000000000001, 74.81, 74.88, 74.71, 74.74, 74.81, 74.83999999999999, 74.77000000000001, 74.65, 74.49, 75.07000000000001, 74.48, 74.79, 75.06, 74.95, 74.83, 74.67, 74.85000000000001, 74.92, 74.82, 74.67, 75.17, 74.81, 74.72, 75.16000000000001, 74.5, 74.92999999999999, 74.7, 75.03, 74.95, 75.01, 74.97, 75.17, 75.07000000000001, 75.21, 75.07000000000001, 75.39, 75.05, 75.4, 75.2, 75.29, 75.38, 75.22, 75.5, 75.48, 75.27000000000001, 75.68, 75.57000000000001, 75.49, 75.39, 75.64999999999999, 75.41, 75.64, 75.33999999999999, 75.56, 75.44999999999999, 75.58, 75.79, 75.77000000000001, 75.82, 75.77000000000001, 75.97, 76.24, 75.86, 75.94999999999999, 75.76, 75.86, 75.92, 76.09, 76.2, 75.94999999999999, 76.02, 76.17, 76.07000000000001, 76.22, 76.1, 76.05, 76.13, 76.1, 76.37, 76.11, 76.38000000000001, 76.14, 76.46, 76.34, 76.29, 76.08, 76.12, 76.3, 76.19, 76.13, 76.14, 76.35, 76.13, 76.31, 76.2, 75.94999999999999, 76.5, 76.44, 76.36, 76.18, 76.22, 76.4, 76.17, 76.44999999999999, 76.33, 76.61, 76.63, 76.74, 76.53999999999999, 76.36, 76.52, 76.37, 76.58, 76.57000000000001, 76.64999999999999, 76.55999999999999, 76.3, 76.68, 76.47, 76.87, 76.63, 76.55, 76.79, 76.57000000000001, 76.77000000000001, 76.88000000000001, 76.9, 76.84, 76.98, 76.82, 76.69, 76.62, 76.67, 76.78, 76.81, 76.88000000000001, 76.87, 76.9, 76.84, 76.84, 77.14, 76.8, 76.77000000000001, 76.97, 76.92, 77.03, 76.84, 77.18, 76.97, 77.32, 76.71, 77.36, 76.88000000000001, 77.38000000000001, 77.03999999999999, 77.27000000000001, 77.16, 77.2, 77.25, 77.23, 77.08, 77.33, 77.18, 77.33, 77.28, 77.25, 77.25999999999999, 77.10000000000001, 77.01, 77.28, 77.10000000000001, 77.17, 77.24, 77.57, 77.08, 77.36, 77.27000000000001, 77.58, 77.32, 77.58, 77.45, 77.5, 77.22, 77.37, 77.03999999999999, 77.31, 77.34, 77.55, 77.47, 77.57, 77.42999999999999, 77.62, 77.42, 77.66, 77.33, 77.69, 77.57, 77.63, 77.53999999999999, 77.75, 77.60000000000001, 77.64999999999999, 77.48, 77.60000000000001, 77.60000000000001, 77.38000000000001, 77.46, 77.52, 77.48, 77.69, 77.38000000000001, 77.69, 77.44, 77.71000000000001, 77.31, 77.78, 77.35, 77.71000000000001, 77.27000000000001, 77.91, 77.69, 77.69, 77.61, 77.77, 77.47, 77.51, 77.58, 77.5, 77.55, 77.75, 77.72, 77.64, 77.91, 77.69, 77.86, 77.75999999999999, 77.7, 77.88000000000001, 77.88000000000001, 77.74, 77.8, 77.75999999999999, 77.94, 77.78, 77.79, 77.95, 77.75, 77.78, 78.05, 77.96, 77.95, 77.9, 77.8, 77.73, 77.89, 77.84, 78.03999999999999, 77.97, 77.95, 77.74, 78.09, 77.82, 77.94, 77.85, 78.01, 77.64999999999999, 78.06, 77.8, 78.19, 77.71000000000001, 78.05, 77.68, 77.86999999999999, 77.73, 77.94, 77.66, 78.06, 77.86, 77.91, 77.92999999999999, 77.79, 77.94, 77.72, 78.09, 77.79, 78.13, 78.0, 78.13, 78.03, 78.13, 77.92999999999999, 78.08, 77.88000000000001, 78.01, 77.79, 78.19, 78.05, 78.08, 77.84, 78.01, 77.92, 78.18, 77.82, 78.18, 77.89, 78.16, 77.69, 78.0, 77.8, 78.03, 77.99000000000001, 77.92999999999999, 77.75999999999999, 78.0, 77.86, 78.03, 77.83, 78.14999999999999, 77.7, 78.07, 77.64999999999999, 77.97, 77.72, 78.07, 77.60000000000001, 78.11, 77.69, 77.96, 77.64, 78.09, 77.78, 78.07, 77.72, 78.0, 77.66, 78.08, 77.95, 78.13, 77.78, 78.01, 77.88000000000001, 78.09, 77.79, 78.25999999999999, 78.02, 78.14999999999999, 77.88000000000001, 78.19, 77.92999999999999, 77.95, 77.9, 78.36, 78.12, 78.03999999999999, 78.10000000000001, 78.05, 78.08, 78.22, 78.08, 78.06, 78.12, 78.16, 78.06, 78.14, 78.0, 78.17, 78.03, 78.38000000000001, 77.89, 78.32000000000001, 77.96, 78.34, 78.01, 78.39, 77.92999999999999, 78.43, 78.21000000000001, 78.27, 78.31, 78.10000000000001, 78.33, 78.18, 78.16, 78.29, 78.18, 78.31, 78.33, 78.45, 78.14999999999999, 78.53999999999999, 78.25999999999999, 78.53999999999999, 78.35, 78.46, 78.36, 78.56, 78.25999999999999, 78.56, 78.19, 78.43, 78.12, 78.51, 78.10000000000001, 78.43, 78.14, 78.46, 78.41, 78.43, 78.18, 78.59, 78.3, 78.56, 78.33, 78.63, 78.36999999999999, 78.33, 78.35, 78.5, 78.33, 78.47, 78.44, 78.31, 78.53, 78.22, 78.36999999999999, 78.29, 78.55, 78.4, 78.42, 78.52, 78.47, 78.47, 78.60000000000001, 78.36, 78.36999999999999, 78.41, 78.32000000000001, 78.55, 78.29, 78.46, 78.38000000000001, 78.49000000000001, 78.46, 78.4, 78.46, 78.44, 78.46, 78.43, 78.38000000000001, 78.53, 78.42, 78.64999999999999, 78.33, 78.47, 78.4, 78.53, 78.60000000000001, 78.5, 78.36999999999999, 78.5, 78.38000000000001, 78.46, 78.32000000000001, 78.43, 78.33, 78.56, 78.43, 78.53999999999999, 78.39, 78.59, 78.53999999999999, 78.67, 78.49000000000001, 78.75999999999999, 78.36, 78.88, 78.61, 78.8, 78.59, 78.67, 78.63, 78.69, 78.85, 78.58000000000001, 78.51, 78.64, 78.58000000000001, 78.62, 78.68, 78.7, 78.49000000000001, 78.69, 78.77, 78.78, 78.53999999999999, 78.81, 78.52, 78.78, 78.73, 78.73, 78.64, 78.75, 78.71000000000001, 78.74, 78.86, 78.73, 78.77, 78.99000000000001, 78.67, 78.91, 78.51, 78.94, 78.56, 78.92, 78.59, 78.79, 78.79, 78.8, 78.68, 78.74, 78.74, 78.77, 78.67, 78.9, 78.60000000000001, 78.83, 78.64999999999999, 78.77, 78.72, 78.81, 78.68, 78.77, 78.79, 78.9, 78.7, 78.97, 78.68, 78.94, 78.7, 78.94, 78.7, 79.03, 78.74, 78.86999999999999, 78.69, 78.92, 78.69, 78.95, 78.8, 78.75999999999999, 78.83, 78.77, 78.7, 78.69, 78.73, 78.75, 78.71000000000001, 78.71000000000001, 78.79, 78.84, 78.75999999999999, 78.93, 78.81, 78.93, 78.82000000000001, 78.93, 78.84, 78.97999999999999, 78.8, 78.88, 78.74, 78.95, 78.75, 79.0, 78.77, 79.08, 78.64, 79.03999999999999, 78.59, 79.03999999999999, 78.68, 79.0, 78.79, 79.09, 78.78, 79.11, 78.89, 78.97999999999999, 78.93, 78.95, 78.86999999999999, 79.0, 78.96, 79.10000000000001, 78.95, 79.07, 78.96, 79.03, 78.97, 79.03, 78.94, 79.0, 79.14999999999999, 79.0, 79.03, 78.92, 78.96, 79.05, 79.07, 79.17, 79.01, 78.97999999999999, 78.93, 79.09, 78.92, 79.01, 78.95, 79.03999999999999, 78.82000000000001, 79.07, 78.9, 79.07, 78.86999999999999, 79.01, 78.97999999999999, 79.17, 79.06, 78.97, 79.06, 79.05, 78.99000000000001, 79.0, 78.88, 79.09, 79.07, 79.09, 78.9, 79.06, 78.97, 79.03999999999999, 79.02, 78.96, 79.05, 79.02, 79.08, 79.07, 78.97999999999999, 79.12, 79.12, 79.07, 78.92, 79.03999999999999, 78.86999999999999, 79.03, 78.89, 79.02, 78.86999999999999, 79.02, 78.97999999999999, 79.05, 78.93, 79.06, 78.94, 79.07, 78.97999999999999, 79.19, 79.05, 79.10000000000001, 79.17, 79.13, 79.16, 79.09, 79.14, 79.11, 79.13, 79.07, 79.22, 79.22, 79.17999999999999, 79.21000000000001, 79.21000000000001, 79.2, 79.2, 79.17999999999999, 79.22, 79.10000000000001, 79.17, 79.14999999999999, 79.31, 79.14999999999999, 79.43, 79.10000000000001, 79.22, 79.02, 79.25, 79.10000000000001, 79.3, 79.25, 79.25, 79.14999999999999, 79.34, 79.25999999999999, 79.25, 79.19, 79.16, 79.25, 79.08, 79.29, 79.07, 79.25999999999999, 79.14, 79.17999999999999, 79.25, 79.17999999999999, 79.25, 79.36, 79.12, 79.25999999999999, 79.21000000000001, 79.32000000000001, 79.22, 79.42, 79.25999999999999, 79.22, 79.19, 79.3, 79.16, 79.3, 79.12, 79.14, 79.17, 79.17, 79.2, 79.32000000000001, 79.21000000000001, 79.32000000000001, 79.17, 79.35, 79.27, 79.09, 79.36, 79.03, 79.34, 79.19, 79.3, 79.25999999999999, 79.33, 79.10000000000001, 79.28, 79.17999999999999, 79.3, 79.32000000000001, 79.36999999999999, 79.3, 79.29, 79.36999999999999, 79.17999999999999, 79.36999999999999, 79.27, 79.31, 79.29, 79.3, 79.38, 79.3, 79.4, 79.33, 79.34, 79.31, 79.25999999999999, 79.33, 79.28, 79.34, 79.44, 79.28, 79.45, 79.22, 79.36, 79.3, 79.43, 79.31, 79.3, 79.27, 79.34, 79.25, 79.42, 79.3, 79.39, 79.28, 79.31, 79.24, 79.25999999999999, 79.17999999999999, 79.25999999999999, 79.19, 79.34, 79.3, 79.33, 79.3, 79.36, 79.17999999999999, 79.33, 79.35, 79.16, 79.33, 79.25999999999999, 79.29, 79.27, 79.29, 79.33, 79.12, 79.32000000000001, 79.06, 79.32000000000001, 79.16, 79.21000000000001, 79.22, 79.29, 79.22, 79.3, 79.25999999999999, 79.35, 79.14, 79.39, 79.3, 79.43, 79.36999999999999, 79.3, 79.3, 79.34, 79.17, 79.42, 79.3, 79.56, 79.3, 79.45, 79.25, 79.47999999999999, 79.17, 79.45, 79.21000000000001, 79.35, 79.17999999999999, 79.44, 79.10000000000001, 79.44, 79.17999999999999, 79.31, 79.35, 79.4, 79.27, 79.41, 79.24, 79.36999999999999, 79.31, 79.42, 79.28, 79.36, 79.32000000000001, 79.32000000000001, 79.29, 79.34, 79.10000000000001, 79.42, 79.14999999999999, 79.28, 79.21000000000001, 79.3, 79.22, 79.34, 79.2, 79.36999999999999, 79.21000000000001, 79.34, 79.14, 79.36999999999999, 79.13, 79.23, 79.05, 79.23, 79.0, 79.3, 79.25, 79.36, 79.14999999999999, 79.3, 79.17, 79.27, 79.29, 79.44, 79.3, 79.38, 79.42, 79.43, 79.4, 79.4, 79.24, 79.36999999999999, 79.32000000000001, 79.33, 79.38, 79.29, 79.28, 79.36999999999999, 79.21000000000001, 79.36999999999999, 79.3, 79.38, 79.34, 79.46, 79.23, 79.43, 79.27, 79.33, 79.31, 79.34, 79.39, 79.39, 79.29, 79.33, 79.33, 79.35, 79.51, 79.45, 79.61, 79.35, 79.64, 79.4, 79.56, 79.36, 79.53, 79.36, 79.54, 79.29, 79.60000000000001, 79.27, 79.69000000000001, 79.44, 79.75999999999999, 79.47999999999999, 79.71000000000001, 79.27, 79.88, 79.32000000000001, 79.91, 79.33, 79.69000000000001, 79.42, 79.81, 79.49000000000001, 79.77, 79.38, 79.55, 79.36999999999999, 79.71000000000001, 79.3, 79.55, 79.27, 79.62, 79.33, 79.63, 79.36, 79.65, 79.34, 79.53, 79.49000000000001, 79.53, 79.46, 79.5, 79.47, 79.45, 79.49000000000001, 79.73, 79.57, 79.74, 79.45, 79.71000000000001, 79.55, 79.7, 79.5, 79.75, 79.67999999999999, 79.61, 79.53, 79.71000000000001, 79.52, 79.66, 79.5, 79.58, 79.60000000000001, 79.51, 79.60000000000001, 79.4, 79.57, 79.42, 79.62, 79.5, 79.56, 79.49000000000001, 79.66, 79.53, 79.62, 79.57, 79.59, 79.65, 79.59, 79.60000000000001, 79.59, 79.58, 79.64, 79.61, 79.55, 79.67999999999999, 79.58, 79.57, 79.64, 79.66, 79.62, 79.58, 79.63, 79.69000000000001, 79.56, 79.74, 79.55, 79.61, 79.64, 79.57, 79.67999999999999, 79.66, 79.59, 79.60000000000001, 79.64, 79.49000000000001, 79.57, 79.51, 79.62, 79.57, 79.64, 79.56, 79.75999999999999, 79.44, 79.66, 79.45, 79.5, 79.55, 79.4, 79.62, 79.57, 79.59, 79.51, 79.58, 79.60000000000001, 79.62, 79.53, 79.55, 79.45, 79.51, 79.47999999999999, 79.51, 79.46, 79.53, 79.46, 79.60000000000001, 79.36999999999999, 79.57, 79.45, 79.59, 79.39, 79.67999999999999, 79.32000000000001, 79.67999999999999, 79.36, 79.57, 79.47, 79.61, 79.57, 79.52, 79.38, 79.63, 79.56, 79.78, 79.55, 79.62, 79.45, 79.67, 79.59, 79.69000000000001, 79.58, 79.71000000000001, 79.61, 79.66, 79.66, 79.79, 79.57, 79.72, 79.62, 79.65, 79.63, 79.61, 79.60000000000001, 79.56, 79.43, 79.64, 79.54, 79.63, 79.54, 79.78, 79.59, 79.7, 79.60000000000001, 79.80000000000001, 79.49000000000001, 79.75999999999999, 79.57, 79.80000000000001, 79.60000000000001, 79.82000000000001, 79.57, 79.92, 79.56, 79.83, 79.60000000000001, 79.86, 79.65, 79.91, 79.69000000000001, 79.97999999999999, 79.75, 79.83, 79.65, 79.91, 79.64, 79.9, 79.60000000000001, 79.94, 79.67, 79.86999999999999, 79.66, 79.88, 79.67, 79.86, 79.64, 79.91, 79.67999999999999, 79.88, 79.58, 79.82000000000001, 79.67999999999999, 79.86, 79.64, 79.95, 79.61, 80.05, 79.71000000000001, 79.99000000000001, 79.71000000000001, 79.96, 79.81, 79.93, 79.67, 79.9, 79.72, 79.97999999999999, 79.67, 79.96, 79.65, 79.97, 79.75, 79.80000000000001, 79.72, 79.86, 79.69000000000001, 79.83, 79.75999999999999, 79.81, 79.66, 79.74, 79.66, 79.83, 79.69000000000001, 79.78, 79.65, 79.85, 79.73, 79.82000000000001, 79.75, 79.89, 79.66, 79.91, 79.83, 79.97, 79.72, 79.95, 79.74, 79.86999999999999, 79.75999999999999, 79.74, 79.84, 79.78, 79.89, 79.72, 79.73, 79.73, 79.71000000000001, 79.77, 79.61, 79.75999999999999, 79.72, 79.71000000000001, 79.77, 79.78, 79.86, 79.93, 79.91, 79.86, 79.97, 79.86999999999999, 79.88, 80.0, 79.83, 79.97999999999999, 79.86999999999999, 79.97, 79.71000000000001, 79.86999999999999, 79.77, 79.75999999999999, 79.73, 79.9, 79.81, 79.86, 79.73, 79.89, 79.7, 79.84, 79.75999999999999, 79.91, 79.79, 79.88, 79.78, 79.84, 79.83, 79.9, 79.86, 79.75999999999999, 79.89, 79.75999999999999, 79.91, 79.75999999999999, 79.84, 79.83, 79.82000000000001, 79.82000000000001, 79.78, 79.94, 79.78, 79.91, 79.78, 79.82000000000001, 79.74, 79.99000000000001, 79.78, 80.04, 79.77, 80.03, 79.80000000000001, 80.02, 79.83, 79.94, 79.72, 80.06, 79.75999999999999, 80.02, 79.73, 80.05, 79.83, 80.08999999999999, 79.80000000000001, 80.06, 79.84, 80.08999999999999, 79.86999999999999, 80.13, 79.97, 80.13, 79.86999999999999, 80.03, 79.97999999999999, 80.10000000000001, 80.01, 80.13, 79.97, 80.13, 79.93, 80.10000000000001, 79.88, 80.01, 79.86, 80.08999999999999, 79.94, 80.06, 79.9, 80.01, 79.91, 79.97999999999999, 79.89, 79.94, 79.92, 79.92, 79.86999999999999, 80.06, 79.86, 79.97999999999999, 79.86999999999999, 79.97999999999999, 79.91, 79.91, 79.94, 79.9, 79.97, 80.05, 79.99000000000001, 80.01, 79.85, 79.97, 79.93, 80.13, 79.97999999999999, 80.03, 80.11, 80.0, 80.02, 80.01, 80.13, 80.01, 80.13, 80.05, 80.08999999999999, 79.97999999999999, 80.04, 80.0, 80.01, 79.99000000000001, 79.89, 79.97999999999999, 80.0, 79.99000000000001, 79.94, 79.88, 79.9, 79.93, 79.93, 79.99000000000001, 79.84, 79.96, 80.01, 79.9, 79.91, 79.95, 79.86, 80.0, 79.96, 79.97999999999999, 79.93, 80.08, 79.83, 80.05, 79.83, 80.04, 79.75999999999999, 80.06, 79.83, 80.08999999999999, 79.92, 80.12, 79.9, 80.10000000000001, 79.92, 80.24, 79.95, 80.14, 79.9, 80.19, 79.99000000000001, 80.17, 80.01, 80.17999999999999, 80.06, 80.08999999999999, 80.04, 80.04, 80.04, 79.99000000000001, 80.05, 80.06, 80.0, 80.07, 80.05, 80.06, 80.02, 80.02, 80.08999999999999, 80.07, 80.04, 80.07, 80.16, 80.05, 80.08, 80.08999999999999, 80.11, 80.07, 80.0, 80.04, 80.11, 80.08999999999999, 80.08999999999999, 80.16, 80.13, 80.19, 80.0, 80.19, 80.07, 80.13, 80.08, 80.24, 80.04, 80.17, 80.07, 80.15, 80.08999999999999, 80.14, 80.08, 80.14, 80.13, 80.2, 80.05, 80.15, 80.10000000000001, 80.12, 80.06, 80.07, 80.01, 80.11, 80.03, 80.10000000000001, 80.10000000000001, 80.08999999999999, 80.03, 80.08, 80.06, 80.10000000000001, 80.02, 80.14, 80.01, 80.04, 80.0, 80.13, 79.93, 80.19, 79.96, 80.04, 79.92, 80.03, 80.02, 80.02, 79.95, 80.02, 79.91, 80.04, 79.94, 79.96, 79.91, 80.0, 79.86, 79.95, 79.93, 79.94, 79.86999999999999, 79.96, 79.81, 80.05, 79.74, 80.02, 79.82000000000001, 80.05, 79.83, 80.04, 79.84, 80.14, 79.9, 80.13, 79.97, 80.08999999999999, 79.96, 79.99000000000001, 79.94, 80.04, 80.02, 80.04, 80.0, 80.01, 80.0, 79.96, 79.97, 79.86, 79.97999999999999, 79.88, 79.97, 79.84, 79.97999999999999, 79.86999999999999, 79.99000000000001, 80.0, 80.01, 79.97999999999999, 79.95, 79.99000000000001, 79.97, 80.06, 79.99000000000001, 80.10000000000001, 79.91, 80.08999999999999, 79.85, 80.05, 79.93, 79.99000000000001, 79.94, 80.03, 79.78, 80.12, 79.82000000000001, 79.97999999999999, 79.89, 80.02, 79.85, 80.04, 79.9, 80.04, 79.78, 80.04, 79.80000000000001, 80.07, 79.69000000000001, 80.12, 79.75, 80.10000000000001, 79.77, 80.06, 79.79, 80.03, 79.72, 80.02, 79.78, 80.01, 79.75, 80.05, 79.80000000000001, 80.08999999999999, 79.86, 80.21000000000001, 79.75999999999999, 80.21000000000001, 79.67999999999999, 80.2, 79.75, 80.23, 79.74, 80.14, 79.79, 80.14, 79.84, 80.2, 79.79, 80.13, 79.86999999999999, 80.08, 79.88, 80.12, 79.88, 80.19, 79.85, 80.12, 79.86999999999999, 80.21000000000001, 79.84, 80.16, 79.86, 80.14, 79.79, 80.22, 79.86999999999999, 80.23, 79.91, 80.24, 79.85, 80.17999999999999, 79.9, 80.33, 79.88, 80.25999999999999, 79.91, 80.25999999999999, 79.95, 80.32000000000001, 79.94, 80.36, 79.93, 80.31, 79.92, 80.4, 79.89, 80.44, 79.86999999999999, 80.31, 79.81, 80.32000000000001, 79.82000000000001, 80.25, 79.92, 80.32000000000001, 79.83, 80.32000000000001, 79.80000000000001, 80.30000000000001, 79.75, 80.25, 79.82000000000001, 80.27, 79.74, 80.2, 79.85, 80.35, 79.95, 80.17999999999999, 79.96, 80.28, 80.0, 80.23, 79.95, 80.15, 80.0, 80.17, 79.96, 80.16, 79.91, 80.08999999999999, 80.05, 80.06, 80.08, 80.08, 80.08, 80.11, 80.06, 80.02, 80.07, 80.10000000000001, 80.05, 80.05, 80.02, 80.01, 80.01, 80.07, 79.99000000000001, 80.0, 79.91, 80.0, 79.93, 80.04, 79.97999999999999, 80.08, 79.91, 80.07, 79.89, 80.05, 79.93, 80.08, 79.99000000000001, 80.06, 80.0, 80.05, 80.04, 80.13, 80.07, 80.17, 80.04, 80.13, 80.10000000000001, 80.23, 80.05, 80.25, 80.08999999999999, 80.13, 80.05, 80.25, 80.16, 80.17, 80.16, 80.23, 80.17999999999999, 80.2, 80.15, 80.25, 80.15, 80.24, 80.12, 80.17, 80.08, 80.16, 80.15, 80.19, 80.2, 80.24, 80.06, 80.34, 80.2, 80.32000000000001, 80.15, 80.28999999999999, 80.14, 80.25, 80.25999999999999, 80.28999999999999, 80.24, 80.13, 80.10000000000001, 80.28, 80.14, 80.19, 80.16, 80.39, 80.14, 80.43, 80.22, 80.49, 80.25, 80.36, 80.22, 80.45, 80.11, 80.39, 80.17, 80.28999999999999, 80.23, 80.34, 80.19, 80.34, 80.13, 80.35, 80.08999999999999, 80.33, 80.17, 80.24, 80.07, 80.27, 80.13, 80.32000000000001, 80.16, 80.36999999999999, 80.25999999999999, 80.32000000000001, 80.07, 80.28, 79.97, 80.38, 80.07, 80.41, 80.08999999999999, 80.44, 80.14, 80.47, 80.08999999999999, 80.46, 80.12, 80.45, 80.07, 80.47999999999999, 80.11, 80.42, 80.02, 80.32000000000001, 79.97999999999999, 80.46, 79.93, 80.49, 79.99000000000001, 80.46, 80.10000000000001, 80.46, 80.08, 80.4, 80.08, 80.39, 80.04, 80.52, 80.13, 80.47999999999999, 80.10000000000001, 80.49, 80.13, 80.51, 80.11, 80.43, 80.17, 80.52, 80.13, 80.46, 80.14, 80.46, 80.08, 80.45, 80.16, 80.42, 80.2, 80.42, 80.17999999999999, 80.47, 80.19, 80.47999999999999, 80.17, 80.47999999999999, 80.16, 80.5, 80.17999999999999, 80.57, 80.15, 80.5, 80.08999999999999, 80.42, 80.21000000000001, 80.45, 80.16, 80.39, 80.10000000000001, 80.55, 80.08, 80.51, 80.03, 80.61, 79.96, 80.57, 80.01, 80.54, 80.07, 80.39, 79.95, 80.51, 80.04, 80.4, 80.22, 80.41, 80.15, 80.41, 80.14, 80.41, 80.06, 80.44, 80.08, 80.47, 80.08999999999999, 80.45, 80.08999999999999, 80.36999999999999, 80.13, 80.43, 80.13, 80.36, 80.14, 80.38, 80.13, 80.36, 80.10000000000001, 80.32000000000001, 80.23, 80.30000000000001, 80.28, 80.33, 80.25, 80.39, 80.23, 80.4, 80.31, 80.22, 80.30000000000001, 80.27, 80.36, 80.28999999999999, 80.14, 80.32000000000001, 80.19, 80.32000000000001, 80.19, 80.31, 80.2, 80.35, 80.16, 80.33, 80.13, 80.28, 80.07, 80.24, 80.08999999999999, 80.24, 80.05, 80.36999999999999, 80.06, 80.32000000000001, 80.03, 80.32000000000001, 80.06, 80.39, 80.07, 80.4, 80.08, 80.32000000000001, 80.11, 80.36999999999999, 80.03, 80.47, 80.08999999999999, 80.46, 80.03, 80.45, 80.08, 80.44, 80.06, 80.45, 80.12, 80.57, 80.02, 80.51, 80.07, 80.55, 80.06, 80.67999999999999, 80.01, 80.57, 80.04, 80.58, 80.10000000000001, 80.55, 79.99000000000001, 80.54, 80.05, 80.53, 80.03, 80.49, 80.03, 80.58, 80.08999999999999, 80.57, 80.02, 80.63, 80.03, 80.55, 80.06, 80.56, 80.02, 80.49, 80.01, 80.56, 80.06, 80.54, 80.03, 80.53, 80.15, 80.51, 80.12, 80.47, 80.11, 80.5, 80.08999999999999, 80.51, 80.12, 80.51, 80.13, 80.55, 80.06, 80.56, 80.03, 80.57, 80.11, 80.57, 80.11, 80.60000000000001, 80.01, 80.58999999999999, 80.04, 80.60000000000001, 79.97999999999999, 80.56, 80.02, 80.55, 79.97999999999999, 80.67999999999999, 79.97, 80.54, 80.01, 80.5, 80.04, 80.49, 80.01, 80.52, 79.94, 80.55, 79.95, 80.58999999999999, 80.12, 80.55, 80.06, 80.56, 80.01, 80.57, 79.88, 80.56, 79.95, 80.51, 80.08999999999999, 80.55, 80.07, 80.49, 80.01, 80.53, 79.99000000000001, 80.56, 80.0, 80.62, 80.01, 80.7, 79.97, 80.7, 79.97, 80.60000000000001, 79.93, 80.52, 79.97999999999999, 80.5, 80.12, 80.47, 80.01, 80.46, 80.07, 80.44, 80.05, 80.44, 80.11, 80.5, 80.04, 80.53, 79.99000000000001, 80.45, 80.08, 80.53, 80.12, 80.51, 80.21000000000001, 80.46, 80.17999999999999, 80.54, 80.2, 80.46, 80.16, 80.44, 80.16, 80.47, 80.21000000000001, 80.39, 80.2, 80.47, 80.17999999999999, 80.46, 80.23, 80.47, 80.21000000000001, 80.44, 80.23, 80.44, 80.22, 80.42, 80.23, 80.4, 80.22, 80.45, 80.17999999999999, 80.51, 80.15, 80.47999999999999, 80.10000000000001, 80.5, 80.15, 80.47, 80.21000000000001, 80.51, 80.21000000000001, 80.54, 80.25999999999999, 80.57, 80.33, 80.55, 80.25, 80.46, 80.30000000000001, 80.33, 80.28, 80.44, 80.17999999999999, 80.53, 80.16, 80.52, 80.12, 80.47, 80.04, 80.49, 80.08, 80.45, 80.21000000000001, 80.47999999999999, 80.25, 80.42, 80.16, 80.36999999999999, 80.25, 80.35, 80.25, 80.33, 80.17999999999999, 80.38, 80.2, 80.36, 80.16, 80.35, 80.14, 80.42, 80.17999999999999, 80.47999999999999, 80.24, 80.46, 80.31, 80.45, 80.31, 80.52, 80.22, 80.51, 80.21000000000001, 80.51, 80.22, 80.60000000000001, 80.28, 80.58999999999999, 80.2, 80.66, 80.24, 80.62, 80.30000000000001, 80.58999999999999, 80.22, 80.55, 80.23, 80.55, 80.14, 80.56, 80.12, 80.51, 80.16, 80.56, 80.13, 80.53, 80.21000000000001, 80.5, 80.07, 80.47999999999999, 80.16, 80.49, 80.19, 80.54, 80.17999999999999, 80.52, 80.17999999999999, 80.5, 80.15, 80.47999999999999, 80.22, 80.44, 80.19, 80.56, 80.25999999999999, 80.57, 80.22, 80.60000000000001, 80.30000000000001, 80.65, 80.24, 80.57, 80.31, 80.54, 80.28999999999999, 80.60000000000001, 80.33, 80.47999999999999, 80.23, 80.53, 80.31, 80.57, 80.32000000000001, 80.57, 80.35, 80.56, 80.30000000000001, 80.58, 80.34, 80.56, 80.30000000000001, 80.52, 80.28, 80.55, 80.33, 80.55, 80.35, 80.55, 80.30000000000001, 80.60000000000001, 80.28, 80.67, 80.31, 80.66, 80.41, 80.71000000000001, 80.36999999999999, 80.74, 80.35, 80.81, 80.30000000000001, 80.75, 80.32000000000001, 80.64, 80.28999999999999, 80.71000000000001, 80.25999999999999, 80.60000000000001, 80.27, 80.56, 80.27, 80.60000000000001, 80.36999999999999, 80.64, 80.33, 80.58999999999999, 80.31, 80.64, 80.28999999999999, 80.57, 80.36999999999999, 80.51, 80.27, 80.55, 80.25999999999999, 80.46, 80.32000000000001, 80.45, 80.35, 80.39, 80.44, 80.4, 80.35, 80.54, 80.38, 80.57, 80.36999999999999, 80.60000000000001, 80.47, 80.58, 80.36, 80.61, 80.46, 80.58, 80.39, 80.5, 80.45, 80.55, 80.38, 80.53, 80.39, 80.61, 80.35, 80.54, 80.25, 80.56, 80.28, 80.71000000000001, 80.30000000000001, 80.67999999999999, 80.33, 80.72, 80.36, 80.72, 80.34, 80.69, 80.31, 80.76, 80.33, 80.67999999999999, 80.34, 80.64, 80.36, 80.58999999999999, 80.44, 80.51, 80.45, 80.60000000000001, 80.49, 80.46, 80.46, 80.46, 80.42, 80.49, 80.4, 80.53, 80.45, 80.52, 80.4, 80.43, 80.4, 80.34, 80.46, 80.5, 80.36999999999999, 80.54, 80.33, 80.53, 80.36, 80.5, 80.25, 80.44, 80.31, 80.47999999999999, 80.31, 80.39, 80.41, 80.34, 80.4, 80.38, 80.33, 80.32000000000001, 80.35, 80.39, 80.31, 80.41, 80.31, 80.35, 80.28999999999999, 80.39, 80.28, 80.39, 80.32000000000001, 80.4, 80.32000000000001, 80.38, 80.36, 80.42, 80.25999999999999, 80.41, 80.23, 80.4, 80.23, 80.36, 80.33, 80.39, 80.36999999999999, 80.39, 80.39, 80.43, 80.30000000000001, 80.38, 80.28999999999999, 80.41, 80.25, 80.39, 80.24, 80.42, 80.27, 80.47999999999999, 80.19, 80.53, 80.25999999999999, 80.5, 80.28999999999999, 80.53, 80.25, 80.47999999999999, 80.25999999999999, 80.58, 80.30000000000001, 80.52, 80.32000000000001, 80.58999999999999, 80.27, 80.52, 80.35, 80.42, 80.35, 80.52, 80.31, 80.51, 80.38, 80.46, 80.30000000000001, 80.49, 80.38, 80.47999999999999, 80.45, 80.43, 80.39, 80.41, 80.42, 80.39, 80.33, 80.46, 80.36, 80.52, 80.39, 80.56, 80.28, 80.61, 80.31, 80.58999999999999, 80.4, 80.57, 80.36999999999999, 80.54, 80.32000000000001, 80.57, 80.44, 80.57, 80.49, 80.5, 80.51, 80.57, 80.4, 80.47, 80.39, 80.49, 80.43, 80.53, 80.4, 80.62, 80.35, 80.71000000000001, 80.41, 80.74, 80.4, 80.65, 80.33, 80.63, 80.33, 80.63, 80.30000000000001, 80.58, 80.31, 80.57, 80.36, 80.57, 80.34, 80.58999999999999, 80.33, 80.62, 80.39, 80.56, 80.36, 80.51, 80.36999999999999, 80.54, 80.4, 80.55, 80.39, 80.47, 80.36, 80.53, 80.39, 80.56, 80.42, 80.60000000000001, 80.36999999999999, 80.62, 80.4, 80.62, 80.45, 80.58999999999999, 80.42, 80.61, 80.41, 80.67999999999999, 80.53, 80.73, 80.5, 80.69, 80.5, 80.73, 80.55, 80.71000000000001, 80.54, 80.65, 80.5, 80.58, 80.46, 80.65, 80.51, 80.63, 80.51, 80.60000000000001, 80.49, 80.65, 80.53, 80.62, 80.5, 80.60000000000001, 80.54, 80.63, 80.5, 80.63, 80.47999999999999, 80.67999999999999, 80.53, 80.67, 80.49, 80.64, 80.52, 80.69, 80.45, 80.64, 80.47, 80.76, 80.43, 80.73, 80.42, 80.72, 80.47, 80.72, 80.5, 80.65, 80.56, 80.64, 80.52, 80.66, 80.55, 80.65, 80.5, 80.65, 80.47999999999999, 80.67, 80.52, 80.71000000000001, 80.53, 80.67, 80.53, 80.65, 80.58, 80.67999999999999, 80.51, 80.67999999999999, 80.56, 80.7, 80.53, 80.69, 80.54, 80.74, 80.5, 80.71000000000001, 80.57, 80.7, 80.46, 80.78, 80.54, 80.82000000000001, 80.51, 80.76, 80.53, 80.76, 80.60000000000001, 80.76, 80.58, 80.71000000000001, 80.60000000000001, 80.71000000000001, 80.57, 80.73, 80.54, 80.69, 80.55, 80.67999999999999, 80.52, 80.71000000000001, 80.49, 80.74, 80.51, 80.72, 80.53, 80.72, 80.56, 80.72, 80.51, 80.69, 80.53, 80.67, 80.54, 80.65, 80.61, 80.62, 80.58999999999999, 80.7, 80.61, 80.60000000000001, 80.58, 80.61, 80.62, 80.62, 80.60000000000001, 80.63, 80.61, 80.66, 80.57, 80.62, 80.53, 80.60000000000001, 80.60000000000001, 80.60000000000001, 80.62, 80.58, 80.58, 80.53, 80.58999999999999, 80.53, 80.58, 80.54, 80.61, 80.53, 80.62, 80.53, 80.64, 80.49, 80.58999999999999, 80.66, 80.62, 80.56, 80.61, 80.62, 80.58999999999999, 80.67, 80.56, 80.7, 80.54, 80.62, 80.55, 80.67, 80.58999999999999, 80.72, 80.58999999999999, 80.76, 80.57, 80.66, 80.61, 80.69, 80.56, 80.65, 80.69, 80.63, 80.67, 80.67, 80.62, 80.7, 80.63, 80.66, 80.62, 80.56, 80.72, 80.47999999999999, 80.63, 80.56, 80.5, 80.51, 80.54, 80.41, 80.65, 80.49, 80.65, 80.36, 80.55, 80.58, 80.53, 80.54, 80.51, 80.57, 80.49, 80.58, 80.58, 80.55, 80.58999999999999, 80.58999999999999, 80.63, 80.63, 80.58999999999999, 80.57, 80.63, 80.58, 80.65, 80.46, 80.62, 80.57, 80.67, 80.54, 80.58, 80.52, 80.56, 80.54, 80.63, 80.62, 80.66, 80.61, 80.65, 80.58, 80.65, 80.63, 80.62, 80.58, 80.56, 80.52, 80.54, 80.57, 80.52, 80.56, 80.63, 80.67999999999999, 80.57, 80.52, 80.47999999999999, 80.63, 80.49, 80.63, 80.49, 80.63, 80.54, 80.67999999999999, 80.63, 80.62, 80.61, 80.63, 80.62, 80.61, 80.65, 80.7, 80.72, 80.66, 80.71000000000001, 80.63, 80.72, 80.58, 80.7, 80.63, 80.71000000000001, 80.65, 80.66, 80.67999999999999, 80.69, 80.72, 80.66, 80.72, 80.69, 80.74, 80.64, 80.62, 80.62, 80.66, 80.71000000000001, 80.69, 80.65, 80.77, 80.69, 80.71000000000001, 80.81, 80.69, 80.81, 80.73, 80.81, 80.7, 80.82000000000001, 80.76, 80.77, 80.7, 80.73, 80.67999999999999]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f56425a3b50>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(accuracy)\n",
        "# plt.plot(error, [i for i in range(NUM_EPOCHS + 1)])\n",
        "plt.plot([i for i in range(NUM_EPOCHS + 1)], error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Rw196p8qugZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "2fa75ad6-b162-42f0-cf34-337696d4a3e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f564251acd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot([i for i in range(NUM_EPOCHS + 1)], accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSnVnBkAkiXf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IST_597 Assignment 2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}